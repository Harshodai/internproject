# InternProject

Hi Welcome to my GitHub
In this Project 
●	Deployed a Batch processing ETL Pipeline, capable of data ingestion, processing and transformation (Masking, Type Casting) through AWS cloud environment, PySpark with a total execution time of 8 minutes.
●	Reduced Formatting of Names, Tasks and Spark Configurations of the Spark Job by using a Configuration File in JSON Format for better Production Code Readability.
●	Reduced the Dataset uploading time to S3 Bucket to less than 30 seconds and Code Reusability with the use of AWS CI/CD Tool - Code Pipeline.
●	Triggered the Job Application using AWS Lambda Triggers, S3 Triggers
●	Deployed and Executed the Spark Job easily without manual operation using Livy Rest API in EMR Cluster
●	Monitored the Entire Job Operation using Scheduling Tools like Apache Airflow using DAGS.

I mainly focussed on creating an ETL Pipeline on AWS Environment
Tech Stack used :
Pyspark,
Amazon S3,
Amazon EC2 Service,
AWS CodePipeline,
Apache Airflow,
Python Faker Library,
AWS Lambda.
